{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9229018,"sourceType":"datasetVersion","datasetId":5577035},{"sourceId":9224605,"sourceType":"datasetVersion","datasetId":5578876}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers pytorch-crf seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrain JointBERT_CRF","metadata":{}},{"cell_type":"code","source":"# !rm -r /kaggle/working/JointBERT-CRF_PhoBERTencoder\n# !rm -r /kaggle/working/JointIDSF_PhoBERTencoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 /kaggle/input/jointidsf/JointIDSF/main.py \\\n                  --token_level word-level \\\n                  --model_type phobert \\\n                  --model_dir JointBERT-CRF_PhoBERTencoder/3e-5/0.6/100 \\\n                  --data_dir /kaggle/input/jointidsf/JointIDSF/PhoATIS \\\n                  --seed 100 \\\n                  --do_train \\\n                  --do_eval \\\n                  --save_steps 140 \\\n                  --logging_steps 140 \\\n                  --num_train_epochs 50 \\\n                  --tuning_metric mean_intent_slot \\\n                  --attention_embedding_size 200 \\\n                  --use_crf \\\n                  --gpu_id 0 \\\n                  --embedding_type soft \\\n                  --intent_loss_coef 0.6 \\\n                  --learning_rate 3e-5 \\\n                  --train_batch_size 32 \\\n                  --attention_mechanism mhsa \\\n                  --optimizer adam \\\n                  --scheduler linearwa \\\n                  --verbose ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## JointISDF","metadata":{}},{"cell_type":"code","source":"# As we initialize JointIDSF from JointBERT, user need to train a base model JointBERT first\n# Train JointIDSF\n!python3 /kaggle/input/jointidsf/JointIDSF/main.py \\\n                  --token_level word-level \\\n                  --model_type phobert \\\n                  --model_dir JointIDSF_PhoBERTencoder/1e-5/0.15/100 \\\n                  --data_dir /kaggle/input/jointidsf/JointIDSF/PhoATIS \\\n                  --seed 100 \\\n                  --do_train \\\n                  --do_eval \\\n                  --save_steps 140 \\\n                  --logging_steps 140 \\\n                  --num_train_epochs 50 \\\n                  --tuning_metric mean_intent_slot \\\n                  --use_intent_context_attention \\\n                  --attention_embedding_size 200 \\\n                  --use_crf \\\n                  --gpu_id 0 \\\n                  --embedding_type soft \\\n                  --intent_loss_coef 0.15 \\\n                  --pretrained \\\n                  --pretrained_path /kaggle/input/jointidsf-mhsa-baselines/JointBERT-CRF_PhoBERTencoder/3e-5/0.6/100 \\\n                  --learning_rate 1e-5 \\\n                  --train_batch_size 32 \\\n                  --attention_mechanism mhsa \\\n                  --optimizer adamw \\\n                  --scheduler cosinewa \\\n                  --num_heads 8 \\\n                  --verbose","metadata":{"execution":{"iopub.status.busy":"2024-08-23T02:32:52.303569Z","iopub.execute_input":"2024-08-23T02:32:52.303949Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nvinai/phobert-base\nSome weights of JointPhoBERT were not initialized from the model checkpoint at /kaggle/input/jointidsf-mhsa-baselines/JointBERT-CRF_PhoBERTencoder/3e-5/0.6/100 and are newly initialized: ['slot_classifier.attention.linear_out.weight', 'slot_classifier.attention.mhsa.in_proj_bias', 'slot_classifier.attention.mhsa.in_proj_weight', 'slot_classifier.attention.mhsa.out_proj.bias', 'slot_classifier.attention.mhsa.out_proj.weight', 'slot_classifier.linear_intent_context.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n0\n0\ncheck init\nEvaluating:   0%|                                         | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorCompare.cpp:530.)\n  score = torch.where(mask[i].unsqueeze(1), next_score, score)\nEvaluating: 100%|█████████████████████████████████| 8/8 [00:01<00:00,  4.16it/s]\n{'loss': 72.1223611831665, 'intent_acc': 0.98, 'slot_precision': 0.00032331070158422246, 'slot_recall': 0.0005837711617046118, 'slot_f1': 0.0004161464835622139, 'semantic_frame_acc': 0.0, 'mean_intent_slot': 0.4902080732417811}\nIteration:   0%|                                        | 0/140 [00:00<?, ?it/s]\nEpoch 0\nIteration:  99%|█████████████████████████████▊| 139/140 [00:29<00:00,  4.74it/s]\nTuning metrics: mean_intent_slot\n\nEvaluating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\nEvaluating:  12%|████▏                            | 1/8 [00:00<00:01,  5.37it/s]\u001b[A\nEvaluating:  25%|████████▎                        | 2/8 [00:00<00:01,  5.54it/s]\u001b[A\nEvaluating:  38%|████████████▍                    | 3/8 [00:00<00:00,  5.58it/s]\u001b[A\nEvaluating:  50%|████████████████▌                | 4/8 [00:00<00:00,  5.58it/s]\u001b[A\nEvaluating:  62%|████████████████████▋            | 5/8 [00:00<00:00,  5.60it/s]\u001b[A\nEvaluating:  75%|████████████████████████▊        | 6/8 [00:01<00:00,  5.63it/s]\u001b[A\nEvaluating:  88%|████████████████████████████▉    | 7/8 [00:01<00:00,  5.65it/s]\u001b[A\nEvaluating: 100%|█████████████████████████████████| 8/8 [00:01<00:00,  5.73it/s]\u001b[A\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n  warnings.warn('{} seems not to be NE tag.'.format(chunk))\nmean_intent_slot increased (inf --> 0.573513).  Saving model ...\n***** Evaluations *****\nLoss/validation 33.16970705986023\nIntent Accuracy/validation 0.89\nSlot F1/validation 0.25702592087312415\nMean Intent Slot 0.5735129604365621\nSentence Accuracy/validation 0.022\n***** ########### *****\nIteration: 100%|██████████████████████████████| 140/140 [00:33<00:00,  4.20it/s]\nIteration:   0%|                                        | 0/140 [00:00<?, ?it/s]\nEpoch 1\nIteration:  99%|█████████████████████████████▊| 139/140 [00:29<00:00,  4.73it/s]\nTuning metrics: mean_intent_slot\n\nEvaluating:   0%|                                         | 0/8 [00:00<?, ?it/s]\u001b[A\nEvaluating:  12%|████▏                            | 1/8 [00:00<00:01,  5.38it/s]\u001b[A\nEvaluating:  25%|████████▎                        | 2/8 [00:00<00:01,  5.58it/s]\u001b[A\nEvaluating:  38%|████████████▍                    | 3/8 [00:00<00:00,  5.64it/s]\u001b[A\nEvaluating:  50%|████████████████▌                | 4/8 [00:00<00:00,  5.66it/s]\u001b[A\nEvaluating:  62%|████████████████████▋            | 5/8 [00:00<00:00,  5.66it/s]\u001b[A\nEvaluating:  75%|████████████████████████▊        | 6/8 [00:01<00:00,  5.68it/s]\u001b[A\nEvaluating:  88%|████████████████████████████▉    | 7/8 [00:01<00:00,  5.70it/s]\u001b[A\nEvaluating: 100%|█████████████████████████████████| 8/8 [00:01<00:00,  5.78it/s]\u001b[A\nmean_intent_slot increased (0.573513 --> 0.794014).  Saving model ...\n***** Evaluations *****\nLoss/validation 18.06252384185791\nIntent Accuracy/validation 0.944\nSlot F1/validation 0.6440287769784172\nMean Intent Slot 0.7940143884892086\nSentence Accuracy/validation 0.308\n***** ########### *****\nIteration: 100%|██████████████████████████████| 140/140 [00:32<00:00,  4.27it/s]\nIteration:   0%|                                        | 0/140 [00:00<?, ?it/s]\nEpoch 2\nIteration:  73%|█████████████████████▊        | 102/140 [00:21<00:07,  4.75it/s]","output_type":"stream"}]}]}